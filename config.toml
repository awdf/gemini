# Main application configuration
LogFile = "app.log"

# Enable or disable debug logging for all modules.
Debug = false 

[ai]
# The API Key for the AI service. Can be set directly or via an environment variable.
# Example: APIKey = "${GOOGLE_API_KEY}"
APIKey = "${GOOGLE_API_KEY}"
Model = "gemini-2.5-flash"
ModelTTS = "gemini-2.5-flash-preview-tts"
Voice = "Kore" # Example voices: Kore, Indi, Luma, Fini, Zola, Sol
VoiceHistory = false #Turn off voice history for best model latency
SystemPrompt = "Role: You are expert in software development. Context: It is your interview process, so respond from 1st person."
TranscriptionPrompt = "Please provide a verbatim transcript of the audio."
VoicePrompt = """Step by step:
Step 1 - Please provide a verbatim transcript of the audio(if exist) as first part of answer in format 'Transcript: <transcript>'.
Step 2 - Determine language of the transcript(if exist) and provide as second part of answer in format 'Language: <language>'.
Step 3 - Generate a concise and accurate response in the same language as the transcript(if exist) as third part of answer in format 'Response: <response>'.

Directives:
1.Never make attention on your expertise or experience, if you are not asked directly.
2.Never respond with code blocks, if you are not asked directly.
3.In case of small talk, you should be pleasant and friendly, otherwise expected your clean professional response.
4.In case of questions about your developer experience or past projects, please check attached to context files.
5.In case of questions about your developer life or your person, find any human like answer.
6.In other cases, play role and show your expertise in software development, science and technology.
"""
Thinking = -1 # Dynamic:-1, None:0, low:512, Medium:8192, High:24576
Thoughts = false
EnableTools = true # Set to true to enable tools like Google Search. This may increase response time and cost.

#Enable it manualy in case of model support cache. Paid tier 1+.
EnableCache = false # Set to true to enable caching. Note: Not all models or API tiers support caching.
CacheDir = "cache" #All files from this path will be added to cache
CacheSystemPrompt = "The following files are provided as context for our conversation."

[ai.retry]
# Configuration for retrying failed API calls.
MaxRetries = 3 # Number of times to retry a failed request after the first attempt.
InitialDelayMs = 1000 # Initial wait time in milliseconds before the first retry.
MaxDelayMs = 10000 # Maximum wait time in milliseconds for exponential backoff.

[vad]
# The normalized RMS level below which we consider the audio to be silent.
SilenceThreshold = 0.02
# The amount of time in seconds to continue recording after the audio level drops below the silence threshold.
HangoverDurationSec = 2.0
# Duration to ignore audio at startup to allow the pipeline to stabilize.
# This prevents initial hardware "pops" from triggering a recording.
WarmupDuration = "2000ms"

[recorder]
# Recordings shorter than this (in bytes) will be deleted.
# Example: 600 000 bytes it is mouse click record size.
MinFileSizeBytes = 600000

[display]
# Width of the RMS volume bar in characters.
BarWidth = 100
# How often to refresh the display in milliseconds.
UpdateIntervalMs = 50

[pipeline]
# Physical sound device for input (Default system device by default)
# List devices: pactl list | grep -A2 'Source #' | grep 'Name: ' | cut -d" " -f2
# Device = "alsa_output.pci-0000_00_1f.3.analog-stereo.monitor"
# Device = "alsa_output.usb-JBL_Quantum_400_0079CM-0265355EM-00.analog-stereo.monitor"
# GStreamer source buffer time in microseconds. Helps prevent race conditions.
BufferTimeUs = 500000